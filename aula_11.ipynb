{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNtQVDtVG9UdVz/xR2yaDI6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Antonyiuri/-conectar-o-arquivo-HTML-com-o-arquivo-de-extens-o-JavaScript/blob/main/aula_11.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N7WyfUxqQ_kN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a406119-343a-4df9-b686-a0f2cb4da9db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install spacy\n",
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydantic\n"
      ],
      "metadata": {
        "id": "T7eDqVPMlRHZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68fdf952-c1e1-4710-f288-72948b95f28b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (2.10.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download 'pt_core_news_lg'\n"
      ],
      "metadata": {
        "id": "8i8d7wpekenM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3936c668-387c-47f8-e3d9-c619ba2c76a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pt-core-news-lg==3.7.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/pt_core_news_lg-3.7.0/pt_core_news_lg-3.7.0-py3-none-any.whl (568.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m568.2/568.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.0 in /usr/local/lib/python3.10/dist-packages (from pt-core-news-lg==3.7.0) (3.7.5)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.0.11)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.15.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (4.66.6)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.10.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.27.1)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2024.8.30)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.20.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (7.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (1.17.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy<3.8.0,>=3.7.0->pt-core-news-lg==3.7.0) (0.1.2)\n",
            "Installing collected packages: pt-core-news-lg\n",
            "Successfully installed pt-core-news-lg-3.7.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('pt_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n"
      ],
      "metadata": {
        "id": "QRo41j0zl-4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load('pt_core_news_lg')"
      ],
      "metadata": {
        "id": "mudV4D3ZmGzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Documento NLP\n",
        "\n"
      ],
      "metadata": {
        "id": "XL2DdR_l2kWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento = nlp(\"As ações do Magazine Luiza S.A., Franca, Brasil, acumularam baixa de 70% ao ano. Assim já devolveram todos, eita.\")\n",
        "# documento = nlp(\"Alegações extraordinárias requerem evidências extraordinárias. Carl Sagan, 'https://carlsagan.com' \")"
      ],
      "metadata": {
        "id": "fg5jUOhunRy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(documento.vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_SPaspqno0y",
        "outputId": "c9699797-a374-4f62-b396-3565b1ddd6bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "371"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(documento))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R-JX91XsnvzS",
        "outputId": "f585d96a-4bf2-4218-dcad-18aaf00dbaff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Token\n"
      ],
      "metadata": {
        "id": "rHZe0DW32p5q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in documento:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bt4mdvPVo5Oc",
        "outputId": "78f8461f-ef24-49cf-f443-823bc733c6e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As\n",
            "ações\n",
            "do\n",
            "Magazine\n",
            "Luiza\n",
            "S.A.\n",
            ",\n",
            "Franca\n",
            ",\n",
            "Brasil\n",
            ",\n",
            "acumularam\n",
            "baixa\n",
            "de\n",
            "70\n",
            "%\n",
            "ao\n",
            "ano\n",
            ".\n",
            "Assim\n",
            "já\n",
            "devolveram\n",
            "todos\n",
            ",\n",
            "eita\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMAXyJ4u2t_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(documento[3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVZ-3uSBqqAc",
        "outputId": "f625c90e-b0c0-4d39-d85f-9695da798f66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magazine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(documento))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HYbF0mZqyud",
        "outputId": "553e3b24-a0d2-424d-e36e-9c28cb2c72d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Caracteristicas dos Tokens"
      ],
      "metadata": {
        "id": "-pOqapW-3qSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens:\", [token.text for token in documento])\n",
        "print(\"Stop word:\", [token.is_stop for token in documento])\n",
        "print(\"Alfanumérico:\", [token.is_alpha for token in documento])\n",
        "print(\"Maìsculo:\", [token.is_upper for token in documento])\n",
        "print(\"Pontuação:\", [token.is_punct for token in documento])\n",
        "print(\"Número:\", [token.like_num for token in documento])\n",
        "print(\"Sentença Inicial:\", [token.is_sent_start for token in documento])"
      ],
      "metadata": {
        "id": "Hus6gl_WrF1-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa781aa4-574f-495e-dc7f-7a9f91fa3e62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['As', 'ações', 'do', 'Magazine', 'Luiza', 'S.A.', ',', 'Franca', ',', 'Brasil', ',', 'acumularam', 'baixa', 'de', '70', '%', 'ao', 'ano', '.', 'Assim', 'já', 'devolveram', 'todos', ',', 'eita', '.']\n",
            "Stop word: [True, False, True, False, False, False, False, False, False, False, False, False, False, True, False, False, True, False, False, True, True, False, True, False, False, False]\n",
            "Alfanumérico: [True, True, True, True, True, False, False, True, False, True, False, True, True, True, False, False, True, True, False, True, True, True, True, False, True, False]\n",
            "Maìsculo: [False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]\n",
            "Pontuação: [False, False, False, False, False, False, True, False, True, False, True, False, False, False, False, True, False, False, True, False, False, False, False, True, False, True]\n",
            "Número: [False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False, False, False, False, False, False]\n",
            "Sentença Inicial: [True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, True, False, False, False, False, False, False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Formato do token"
      ],
      "metadata": {
        "id": "fiA8HyVF34si"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens:\", [token.text for token in documento])\n",
        "print(\"Formato:\", [token.shape_ for token in documento])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_nsE9tm2sezJ",
        "outputId": "5896b7a0-7ac7-4a30-c621-be76b4901a62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['As', 'ações', 'do', 'Magazine', 'Luiza', 'S.A.', ',', 'Franca', ',', 'Brasil', ',', 'acumularam', 'baixa', 'de', '70', '%', 'ao', 'ano', '.', 'Assim', 'já', 'devolveram', 'todos', ',', 'eita', '.']\n",
            "Formato: ['Xx', 'xxxx', 'xx', 'Xxxxx', 'Xxxxx', 'X.X.', ',', 'Xxxxx', ',', 'Xxxxx', ',', 'xxxx', 'xxxx', 'xx', 'dd', '%', 'xx', 'xxx', '.', 'Xxxxx', 'xx', 'xxxx', 'xxxx', ',', 'xxxx', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jVl0u2IM39Fi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Encontra pontução e números"
      ],
      "metadata": {
        "id": "WU7-yzK04NmS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in documento:\n",
        "    if token.like_num:\n",
        "        print('Numero encontrato:', token.text)\n",
        "    if token.is_punct:\n",
        "        print('Pontuação encontrada:', token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cs5Hzrfvs__Q",
        "outputId": "123a36b5-26ed-4902-a082-9e0ed0a39be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pontuação encontrada: ,\n",
            "Pontuação encontrada: ,\n",
            "Pontuação encontrada: ,\n",
            "Numero encontrato: 70\n",
            "Pontuação encontrada: %\n",
            "Pontuação encontrada: .\n",
            "Pontuação encontrada: ,\n",
            "Pontuação encontrada: .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sZRSVfEd4X67"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in documento:\n",
        "  print(token.text, \"-\", token.pos_, \" - \", token.dep_, \"-\", token.lemma_ , \"-\", token.shape_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HCL1VL6VWngA",
        "outputId": "d1693ead-2060-4c30-c63f-b0c56755309a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As - DET  -  det - o - Xx\n",
            "ações - NOUN  -  nsubj - ação - xxxx\n",
            "do - ADP  -  case - de o - xx\n",
            "Magazine - PROPN  -  nmod - Magazine - Xxxxx\n",
            "Luiza - PROPN  -  appos - Luiza - Xxxxx\n",
            "S.A. - PROPN  -  flat:name - S.A. - X.X.\n",
            ", - PUNCT  -  punct - , - ,\n",
            "Franca - PROPN  -  conj - Franca - Xxxxx\n",
            ", - PUNCT  -  punct - , - ,\n",
            "Brasil - PROPN  -  conj - Brasil - Xxxxx\n",
            ", - PUNCT  -  punct - , - ,\n",
            "acumularam - VERB  -  ROOT - acumular - xxxx\n",
            "baixa - NOUN  -  obj - baixa - xxxx\n",
            "de - ADP  -  case - de - xx\n",
            "70 - NUM  -  nummod - 70 - dd\n",
            "% - SYM  -  nmod - % - %\n",
            "ao - ADP  -  case - a o - xx\n",
            "ano - NOUN  -  nmod - ano - xxx\n",
            ". - PUNCT  -  punct - . - .\n",
            "Assim - ADV  -  advmod - assim - Xxxxx\n",
            "já - ADV  -  advmod - já - xx\n",
            "devolveram - VERB  -  ROOT - devolver - xxxx\n",
            "todos - PRON  -  obj - todo - xxxx\n",
            ", - PUNCT  -  punct - , - ,\n",
            "eita - NOUN  -  conj - eitar - xxxx\n",
            ". - PUNCT  -  punct - . - .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "3EdiIGTyYt9J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in documento:\n",
        "  print(token.text, \"-\", token.morph)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U8pugoYbVRE3",
        "outputId": "58837f07-3d73-4e8c-fdd3-4381e2d219c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As - Definite=Def|Gender=Fem|Number=Plur|PronType=Art\n",
            "ações - Gender=Fem|Number=Plur\n",
            "do - Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
            "Magazine - Gender=Fem|Number=Sing\n",
            "Luiza - Gender=Fem|Number=Sing\n",
            "S.A. - Number=Sing\n",
            ", - \n",
            "Franca - Gender=Fem|Number=Sing\n",
            ", - \n",
            "Brasil - Gender=Masc|Number=Sing\n",
            ", - \n",
            "acumularam - Mood=Ind|Number=Plur|Person=3|Tense=Past|VerbForm=Fin\n",
            "baixa - Gender=Fem|Number=Sing\n",
            "de - \n",
            "70 - NumType=Card\n",
            "% - \n",
            "ao - Definite=Def|Gender=Masc|Number=Sing|PronType=Art\n",
            "ano - Gender=Masc|Number=Sing\n",
            ". - \n",
            "Assim - \n",
            "já - \n",
            "devolveram - Mood=Ind|Number=Plur|Person=3|Tense=Past|VerbForm=Fin\n",
            "todos - Gender=Masc|Number=Plur|PronType=Tot\n",
            ", - \n",
            "eita - Gender=Fem|Number=Sing\n",
            ". - \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0NEKWlM3YvLq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in documento:\n",
        "  print(token.text, \"-\", token.tag_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ih_RzlHTWsj0",
        "outputId": "72a23e59-70f6-4ced-aa81-002801a217d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As - DET\n",
            "ações - NOUN\n",
            "do - ADP\n",
            "Magazine - PROPN\n",
            "Luiza - PROPN\n",
            "S.A. - PROPN\n",
            ", - PUNCT\n",
            "Franca - PROPN\n",
            ", - PUNCT\n",
            "Brasil - PROPN\n",
            ", - PUNCT\n",
            "acumularam - VERB\n",
            "baixa - NOUN\n",
            "de - ADP\n",
            "70 - NUM\n",
            "% - SYM\n",
            "ao - ADP\n",
            "ano - NOUN\n",
            ". - PUNCT\n",
            "Assim - ADV\n",
            "já - ADV\n",
            "devolveram - VERB\n",
            "todos - PRON\n",
            ", - PUNCT\n",
            "eita - NOUN\n",
            ". - PUNCT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "i8jJsqE4YwtQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in documento.ents:\n",
        "  print(ent.text, \"-\", ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czTommAHa4JT",
        "outputId": "6053d1aa-51dd-4be9-8b40-881b6d0f0f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Magazine Luiza S.A. - ORG\n",
            "Franca - LOC\n",
            "Brasil - LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xiPx2dFPYxRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in documento:\n",
        "  if token.is_stop:\n",
        "    print(\"Stop word\", token.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__2NqoyXcXIQ",
        "outputId": "b3bb085b-aef7-415d-c239-4a00b98501b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stop word As\n",
            "Stop word do\n",
            "Stop word de\n",
            "Stop word ao\n",
            "Stop word Assim\n",
            "Stop word já\n",
            "Stop word todos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jRCx5RSNYyHB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for words in nlp.Defaults.stop_words:\n",
        "  print(words)"
      ],
      "metadata": {
        "id": "d9HxcIuNcva3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b70a2c3b-5148-4cd2-b31e-f28084d70da6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "estivemos\n",
            "enquanto\n",
            "dar\n",
            "mas\n",
            "cima\n",
            "dão\n",
            "ter\n",
            "às\n",
            "tivestes\n",
            "faço\n",
            "esta\n",
            "comprido\n",
            "onze\n",
            "nova\n",
            "próprio\n",
            "a\n",
            "ver\n",
            "numa\n",
            "baixo\n",
            "número\n",
            "és\n",
            "possível\n",
            "tens\n",
            "seu\n",
            "estes\n",
            "comprida\n",
            "contudo\n",
            "irá\n",
            "oitavo\n",
            "porquê\n",
            "tais\n",
            "fazer\n",
            "breve\n",
            "poder\n",
            "sabe\n",
            "dezoito\n",
            "estas\n",
            "mais\n",
            "segundo\n",
            "onde\n",
            "estás\n",
            "corrente\n",
            "quieta\n",
            "conselho\n",
            "nessa\n",
            "aqui\n",
            "quê\n",
            "quinto\n",
            "sou\n",
            "essa\n",
            "minhas\n",
            "qual\n",
            "porquanto\n",
            "naquele\n",
            "quem\n",
            "nos\n",
            "custa\n",
            "tudo\n",
            "mês\n",
            "quarto\n",
            "nossos\n",
            "fomos\n",
            "te\n",
            "obrigado\n",
            "o\n",
            "do\n",
            "vários\n",
            "vão\n",
            "doze\n",
            "cujo\n",
            "certeza\n",
            "mal\n",
            "era\n",
            "portanto\n",
            "grupo\n",
            "tentar\n",
            "umas\n",
            "depois\n",
            "me\n",
            "alguns\n",
            "agora\n",
            "tua\n",
            "nesse\n",
            "tenho\n",
            "podia\n",
            "ser\n",
            "cedo\n",
            "diz\n",
            "fazeis\n",
            "nuns\n",
            "da\n",
            "tu\n",
            "nossas\n",
            "tarde\n",
            "essas\n",
            "os\n",
            "ligado\n",
            "quero\n",
            "demais\n",
            "eu\n",
            "aquela\n",
            "quarta\n",
            "isto\n",
            "lado\n",
            "vos\n",
            "local\n",
            "através\n",
            "seis\n",
            "estava\n",
            "estiveste\n",
            "todas\n",
            "ou\n",
            "bom\n",
            "bastante\n",
            "forma\n",
            "fazes\n",
            "tempo\n",
            "quieto\n",
            "eventual\n",
            "tanta\n",
            "vossos\n",
            "dessa\n",
            "porque\n",
            "relação\n",
            "vai\n",
            "deste\n",
            "estou\n",
            "foi\n",
            "possivelmente\n",
            "posso\n",
            "puderam\n",
            "exemplo\n",
            "vossas\n",
            "outras\n",
            "apenas\n",
            "aquelas\n",
            "estará\n",
            "após\n",
            "na\n",
            "pouco\n",
            "desse\n",
            "lá\n",
            "dois\n",
            "outra\n",
            "quais\n",
            "tive\n",
            "daquela\n",
            "seria\n",
            "grandes\n",
            "põem\n",
            "toda\n",
            "caminho\n",
            "sétimo\n",
            "que\n",
            "tiveste\n",
            "porém\n",
            "máximo\n",
            "estive\n",
            "quanto\n",
            "tuas\n",
            "povo\n",
            "devem\n",
            "adeus\n",
            "pontos\n",
            "além\n",
            "perto\n",
            "algo\n",
            "desta\n",
            "teve\n",
            "estão\n",
            "aquele\n",
            "partir\n",
            "pois\n",
            "vinte\n",
            "de\n",
            "meio\n",
            "foram\n",
            "deve\n",
            "já\n",
            "favor\n",
            "esses\n",
            "vezes\n",
            "qualquer\n",
            "ademais\n",
            "e\n",
            "fim\n",
            "vossa\n",
            "podem\n",
            "não\n",
            "naquela\n",
            "dezasseis\n",
            "terceira\n",
            "teus\n",
            "ora\n",
            "fazem\n",
            "só\n",
            "dentro\n",
            "antes\n",
            "bem\n",
            "próximo\n",
            "debaixo\n",
            "saber\n",
            "tentei\n",
            "aqueles\n",
            "dos\n",
            "das\n",
            "novas\n",
            "esse\n",
            "dezassete\n",
            "inicio\n",
            "ambas\n",
            "próxima\n",
            "falta\n",
            "fará\n",
            "dá\n",
            "em\n",
            "outros\n",
            "dezanove\n",
            "todos\n",
            "usar\n",
            "cada\n",
            "cento\n",
            "conhecida\n",
            "maiorias\n",
            "nem\n",
            "nada\n",
            "estiveram\n",
            "cuja\n",
            "tanto\n",
            "fazia\n",
            "uns\n",
            "desde\n",
            "sete\n",
            "des\n",
            "último\n",
            "três\n",
            "muitos\n",
            "novo\n",
            "aí\n",
            "vinda\n",
            "ponto\n",
            "lugar\n",
            "nosso\n",
            "nunca\n",
            "tente\n",
            "sua\n",
            "ainda\n",
            "sistema\n",
            "pelo\n",
            "primeiro\n",
            "como\n",
            "tem\n",
            "estivestes\n",
            "segunda\n",
            "sob\n",
            "pelas\n",
            "iniciar\n",
            "dizem\n",
            "conhecido\n",
            "você\n",
            "usa\n",
            "certamente\n",
            "mil\n",
            "também\n",
            "embora\n",
            "deverá\n",
            "inclusive\n",
            "cá\n",
            "primeira\n",
            "seus\n",
            "esteve\n",
            "são\n",
            "quinta\n",
            "maioria\n",
            "zero\n",
            "área\n",
            "neste\n",
            "disso\n",
            "querem\n",
            "quinze\n",
            "este\n",
            "terceiro\n",
            "algumas\n",
            "então\n",
            "têm\n",
            "aos\n",
            "logo\n",
            "somos\n",
            "tiveram\n",
            "faz\n",
            "nove\n",
            "apoio\n",
            "à\n",
            "catorze\n",
            "fez\n",
            "parte\n",
            "direita\n",
            "vocês\n",
            "eles\n",
            "sexta\n",
            "valor\n",
            "muito\n",
            "entre\n",
            "menos\n",
            "pelos\n",
            "estado\n",
            "ir\n",
            "sei\n",
            "assim\n",
            "questão\n",
            "se\n",
            "quer\n",
            "oito\n",
            "ontem\n",
            "cinco\n",
            "dizer\n",
            "momento\n",
            "longe\n",
            "tipo\n",
            "suas\n",
            "sétima\n",
            "números\n",
            "quando\n",
            "final\n",
            "tivemos\n",
            "ele\n",
            "teu\n",
            "nossa\n",
            "meus\n",
            "aquilo\n",
            "for\n",
            "atrás\n",
            "tentaram\n",
            "uma\n",
            "talvez\n",
            "num\n",
            "pela\n",
            "sempre\n",
            "fora\n",
            "poderá\n",
            "vosso\n",
            "novos\n",
            "vais\n",
            "pouca\n",
            "pôde\n",
            "nós\n",
            "minha\n",
            "com\n",
            "vós\n",
            "quatro\n",
            "fui\n",
            "é\n",
            "nas\n",
            "pegar\n",
            "somente\n",
            "meses\n",
            "ao\n",
            "estar\n",
            "no\n",
            "tão\n",
            "tendes\n",
            "treze\n",
            "foste\n",
            "nenhuma\n",
            "nível\n",
            "daquele\n",
            "menor\n",
            "sexto\n",
            "obrigada\n",
            "dez\n",
            "duas\n",
            "vem\n",
            "ali\n",
            "sem\n",
            "um\n",
            "vens\n",
            "diante\n",
            "põe\n",
            "apontar\n",
            "nesta\n",
            "sobre\n",
            "lhe\n",
            "apoia\n",
            "as\n",
            "ambos\n",
            "todo\n",
            "mesmo\n",
            "veja\n",
            "pode\n",
            "fostes\n",
            "parece\n",
            "para\n",
            "vez\n",
            "contra\n",
            "sim\n",
            "boa\n",
            "maior\n",
            "ela\n",
            "vindo\n",
            "meu\n",
            "vêm\n",
            "tal\n",
            "está\n",
            "por\n",
            "oitava\n",
            "posição\n",
            "elas\n",
            "até\n",
            "isso\n",
            "sois\n",
            "grande\n",
            "fazemos\n",
            "acerca\n",
            "temos\n",
            "geral\n",
            "coisa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.Defaults.stop_words.add(\"eita\")\n",
        "nlp.vocab[\"eita\"].is_stop = True"
      ],
      "metadata": {
        "id": "Ib1RHJXTdHes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.vocab[\"eita\"].is_stop"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQmljxC_dcb7",
        "outputId": "b03846ed-79d1-42c2-faaf-4e5e1591a1aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_lista = []\n",
        "for token in documento:\n",
        "  token_lista.append(token.text)\n",
        "\n",
        "stop_lista = []\n",
        "for words in nlp.Defaults.stop_words:\n",
        "  stop_lista.append(words)\n",
        "\n",
        "semstop = [word for word in token_lista if not word in stop_lista]\n",
        "\n",
        "print(documento.text)\n",
        "print(semstop)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yb3kpounerTh",
        "outputId": "4dc78644-92f4-4b6d-c644-94e405f86941"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "As ações do Magazine Luiza S.A., Franca, Brasil, acumularam baixa de 70% ao ano. Assim já devolveram todos, eita.\n",
            "['As', 'ações', 'Magazine', 'Luiza', 'S.A.', ',', 'Franca', ',', 'Brasil', ',', 'acumularam', 'baixa', '70', '%', 'ano', '.', 'Assim', 'devolveram', ',', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Hash da palavra\n"
      ],
      "metadata": {
        "id": "x-Ch_PKO2JEU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Hash: \", nlp.vocab.strings[\"dados\"])\n",
        "print(\"Hash: \", documento.vocab.strings[\"dados\"])\n",
        "print(\"String\", nlp.vocab.strings[6013848609874238634])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9gWxplK1ysS7",
        "outputId": "cd9c84f4-9837-4a15-df84-d326cb9c8712"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hash:  6013848609874238634\n",
            "Hash:  6013848609874238634\n",
            "String dados\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lex = nlp.vocab[\"dados\"]\n",
        "print(lex.text, \"-\", lex.orth, \"-\", lex.is_alpha, \"-\", lex.is_lower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUjFp2MJ0Sar",
        "outputId": "3b97501f-2d0a-4651-b64f-bd4b189dc249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dados - 6013848609874238634 - True - True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vetores"
      ],
      "metadata": {
        "id": "mLW7RCC61-1t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(nlp(\"dados\").vector.shape)\n",
        "print(nlp(\"dados\").vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2SEsVqc1TdT",
        "outputId": "c071bf2a-00d4-405b-8edc-2c38f1ac4779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(300,)\n",
            "[-1.8078e+00 -2.8421e+00 -5.5345e-01 -4.0565e+00 -2.3565e+00  4.7295e+00\n",
            " -3.3508e+00 -1.2036e+00 -5.3749e+00 -7.5768e-01 -2.0015e+00  5.3618e+00\n",
            "  2.2789e-01  1.2136e+00 -1.4418e+00 -3.0621e+00  2.2060e+00  1.1943e+00\n",
            " -1.8689e+00 -5.9128e+00 -5.1929e-01  2.1897e+00 -2.4651e+00  1.7600e+00\n",
            " -5.6308e+00  1.0053e+00  7.3080e-01  2.0417e-01 -6.0370e+00  1.7474e+00\n",
            "  2.0375e+00  2.7541e+00 -1.0423e+00  1.7632e+00 -1.4484e+00 -2.4344e+00\n",
            " -8.6094e-01  1.7867e+00 -4.9920e+00 -3.1162e+00 -5.3973e-01  2.4241e+00\n",
            "  4.4410e-01 -4.7503e+00  2.1383e+00  3.8537e-01 -2.2418e+00  5.8104e+00\n",
            " -1.1731e+00  2.4686e-02 -1.8413e+00 -5.9321e+00 -7.0872e-03 -2.3093e+00\n",
            "  5.5101e+00 -4.1795e-01  2.9754e+00  3.6709e-01 -8.0867e-01 -1.3751e+00\n",
            "  4.8132e+00 -2.2835e+00 -2.0920e-01  1.2578e+00  4.3466e+00  1.3846e+00\n",
            " -1.5847e+00  3.3322e+00 -4.3669e+00  1.8883e+00 -1.2312e+00  3.7902e-01\n",
            "  8.6331e-01 -4.0607e+00 -1.3444e+00  4.5210e+00  2.2995e-01 -5.9245e+00\n",
            " -2.0598e-01 -3.1810e+00 -3.8640e+00  7.2114e+00  3.1034e+00 -4.2856e+00\n",
            "  5.4147e+00 -7.2224e+00  4.3610e+00 -2.6696e+00  1.9245e+00 -2.6187e+00\n",
            " -5.1742e+00  1.0513e-01  1.9628e+00  1.0356e+00  6.8692e+00 -7.3888e-01\n",
            "  3.8783e+00  4.1958e+00 -8.8872e+00 -1.8452e+00  5.2786e+00  4.2920e+00\n",
            " -4.2204e-01  1.0009e+00  4.2165e-01  1.1927e+00 -8.4580e-01 -2.6577e+00\n",
            "  1.5024e+00  4.4229e-01 -2.5251e+00 -7.1781e-01  7.0878e-01  1.7207e+00\n",
            "  3.8369e+00 -1.5924e+00  2.0870e+00  4.3324e-01 -1.0600e+00  2.7058e+00\n",
            "  1.3282e+00 -7.1241e+00 -1.3663e+00 -3.0763e+00  2.7714e+00  1.3094e+00\n",
            "  1.6072e+00  3.6821e+00  5.3843e+00 -3.5457e-02 -2.9616e+00  1.5253e+00\n",
            "  2.0812e-01  6.0771e-01 -5.5709e-01  7.5687e+00 -9.1431e-01 -3.0477e+00\n",
            "  1.2046e+00  5.3871e-01  5.7112e-01  2.2039e+00 -5.1948e-02 -2.6353e+00\n",
            " -2.8517e+00  1.8675e-01  2.1013e+00  7.5272e-01 -8.7856e+00 -1.4493e+00\n",
            " -7.9085e-01 -1.6936e+00  1.4793e-02 -9.9357e+00  3.7818e+00 -1.7839e+00\n",
            "  1.5425e+00 -3.0899e-02 -5.1442e+00  4.2366e-01  3.2065e+00  3.9226e+00\n",
            " -1.4392e-02  1.5862e+00  7.0632e+00  7.5689e+00 -3.4420e+00 -6.1376e+00\n",
            "  5.0851e-01 -6.3689e-01 -1.8791e-01  1.9850e+00  3.0888e+00 -3.0506e+00\n",
            "  1.1851e-01 -1.2290e+00  7.6061e+00  1.8492e-01 -2.7509e+00 -1.3448e+00\n",
            "  1.9857e+00 -2.4650e+00 -2.0810e+00  5.6084e+00 -7.3379e-01  1.1205e+00\n",
            "  1.3258e-02 -7.0243e-01  1.0118e+00  3.8903e+00 -1.2631e+00 -2.7544e+00\n",
            "  9.3134e-01  2.7516e+00 -1.6767e+00  4.5501e+00 -4.9452e-01  1.0644e+00\n",
            " -1.2653e+00  3.9671e-01 -6.4705e+00 -7.7726e-01 -3.9283e+00  7.9536e+00\n",
            "  2.0967e+00  1.2104e+00  3.4474e+00 -2.2200e+00 -4.9974e+00  3.1827e-01\n",
            " -1.0450e+00 -2.1353e+00  1.0529e+00 -1.6568e+00 -3.0012e+00  4.2834e+00\n",
            "  6.9626e-01  4.5401e+00 -1.7153e-01 -5.9982e-01  3.6147e+00 -3.4545e+00\n",
            "  3.6811e-01 -4.1399e+00 -1.8144e+00  2.9088e+00  8.9164e-01 -2.4783e+00\n",
            " -3.6413e+00 -3.4140e-01 -8.0959e-01 -4.7451e+00  4.2487e+00  6.6012e+00\n",
            " -5.1022e+00  1.5260e+00  3.5160e+00 -1.4569e+00  1.4442e+00  6.8183e+00\n",
            " -5.0775e+00  1.0688e+00  4.8081e+00  3.6586e+00 -3.0766e+00 -8.5266e-01\n",
            " -3.3401e+00 -5.2864e+00 -9.1773e-01 -1.8197e+00  2.0192e+00 -6.7857e+00\n",
            " -3.6542e+00 -3.4654e+00  3.3276e+00  9.0340e-02 -3.0743e+00  3.3329e+00\n",
            " -2.6458e+00  2.6557e-02 -4.7538e-01  2.7974e+00  5.3304e+00  2.9937e+00\n",
            "  3.8899e+00 -3.3433e+00 -3.4940e+00  2.1821e+00  8.1387e+00  3.5272e-01\n",
            "  4.8159e+00 -7.8327e-01 -1.2468e+00  5.9163e+00 -6.0636e-02 -4.3255e-01\n",
            " -1.2690e+00 -3.2938e+00  3.2941e-02 -9.3676e-01 -7.1630e-01  1.4011e+00\n",
            "  1.8334e+00 -1.4261e+00 -1.9903e+00 -1.4721e+00  2.1466e+00 -2.0949e+00\n",
            " -7.6855e-01 -2.8615e+00 -2.0466e+00  7.1870e-01 -1.1195e+00 -3.1117e+00\n",
            "  1.5789e-01 -4.0320e+00 -9.2665e-01 -6.2468e-01 -2.7397e+00  6.0866e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tecnica de similaridade"
      ],
      "metadata": {
        "id": "-u3Gh2e9YCQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento1 = nlp(\"Ele viaja regularmente de carro\")\n",
        "documento2 = nlp(\"Ela viaja regularmente de avião\")\n",
        "print(documento1.similarity(documento2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5OFqoBfUkpa",
        "outputId": "a2b7a194-149e-4d70-a661-cc156da465c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.894174124950273\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Busca de similaridade por palavra"
      ],
      "metadata": {
        "id": "rNXzH0kQY3E9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento3 = nlp(\"Devemos dizer comprimento ou cumprimento?\")\n",
        "tokenA = documento3[2]\n",
        "print(tokenA)\n",
        "tokenB = documento3[4]\n",
        "print(tokenB)\n",
        "print(tokenA.similarity(tokenB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiqeav6BY3e1",
        "outputId": "0d23b70a-6629-493a-a7a5-14c4cae4f356"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comprimento\n",
            "cumprimento\n",
            "0.5804340243339539\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Busca de similaridade por frase"
      ],
      "metadata": {
        "id": "azpVCQqsZuZ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "documento4 = nlp(\"Ele pede descrição. Ele pede discrição\")\n",
        "partA = documento4[0:3]\n",
        "print(partA)\n",
        "partB = documento4[4:7]\n",
        "print(partB)\n",
        "print(partA.similarity(partB))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41X2lz14Zu0c",
        "outputId": "86e50b2b-d026-48ea-f96a-6e1f60ceb293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ele pede descrição\n",
            "Ele pede discrição\n",
            "0.9630053639411926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k5duilrIa9xS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zLnPUkz3a8C7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "import spacy\n",
        "# Inicializa o modelo de linguagem do spaCy\n",
        "nlp = spacy.blank(\"pt\")\n",
        "# Inicializa o matcher com o vocabulário\n",
        "matcher = Matcher(nlp.vocab)\n",
        "#Adiciona padrão para telefone (41) - 778778877\n",
        "padrao = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"dd\"}, {\"ORTH\": \")\"}, {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
        "            {\"IS_DIGIT\": True}]\n",
        "matcher.add(\"telefone\", [padrao])\n",
        "\n",
        "documento5 = nlp(\"Você pode ligar para (51) - 9964656570 ou (11) 12344988\")\n",
        "\n",
        "matches = matcher(documento5)\n",
        "for match_id, start, end in matches:\n",
        "  print(documento5[start:end])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZbSY-jD-ehK",
        "outputId": "37d32bd2-a3da-4628-ffc1-0ac675f764d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(51) - 9964656570\n",
            "(11) 12344988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "import spacy\n",
        "\n",
        "documento5 = nlp(\"Você pode ligar para (51) - 9964656570 ou (11) 12344988\")\n",
        "\n",
        "matcher = Matcher(nlp.vocab)\n",
        "padrao = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"dd\"}, {\"ORTH\": \")\"}, {\"ORTH\": \"-\", \"OP\": \"?\"},\n",
        "            {\"IS_DIGIT\": True}]\n",
        "matcher.add(\"telefone\", [padrao])\n",
        "matches = matcher(documento5)\n",
        "for id, inicio, fim in matches:\n",
        "  print(documento5[inicio:fim])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4M0YPJYUXXk",
        "outputId": "5c3e5833-48d2-4daa-c7a9-b54612299b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(51) - 9964656570\n",
            "(11) 12344988\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documento6 = nlp(\"Estamos infectados com micro organismos. MICROORGANISMOS são perigosos. Não enxergamos micro-organismos\")\n",
        "matcher = Matcher(nlp.vocab)\n",
        "padrao1 = [{\"LOWER\": \"micro-organismos\"}]\n",
        "padrao2 = [{\"LOWER\": \"microorganismos\"}]\n",
        "padrao3 = [{\"LOWER\": \"micro\"}, {\"LOWER\": \"organismos\"}]\n",
        "matcher.add(\"padrao\", [padrao1, padrao2, padrao3])\n",
        "\n",
        "matches = matcher(documento6)\n",
        "for id, inicio, fim in matches:\n",
        "  print(documento6[inicio:fim])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nofLT330X3-u",
        "outputId": "585b4906-717c-4e2b-9478-8fd2473f208a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "micro organismos\n",
            "MICROORGANISMOS\n",
            "micro-organismos\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(documento, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "UF0JLRMrfdu6",
        "outputId": "61e3f2f5-d3b9-4dec-e0c6-9aced6571826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">As ações do \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Magazine Luiza S.A.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Franca\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ", \n",
              "<mark class=\"entity\" style=\"background: #ff9561; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Brasil\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">LOC</span>\n",
              "</mark>\n",
              ", acumularam baixa de 70% ao ano. Assim já devolveram todos, eita.</div></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "documento.user_data[\"Estudo\"] = \"Documento NLP\"\n",
        "displacy.render(documento, style=\"dep\", jupyter=True,\n",
        "                options={'compact': False, \"distance\": 80,'color': '#FFFFE0', 'bg': '#00000', 'font': 'Arial' })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        },
        "id": "PclOk92uhOff",
        "outputId": "975e96ce-2fa5-493d-d1cc-b893f2f38595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"pt\" id=\"340775e57957462ea1bf746618bb93d0-0\" class=\"displacy\" width=\"1570\" height=\"337.0\" direction=\"ltr\" style=\"max-width: none; height: 337.0px; color: #FFFFE0; background: #00000; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">As</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"130\">ações</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"130\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"210\">do</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"210\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">Magazine</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"370\">Luiza</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"370\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">S.A.,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">Franca,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"610\">Brasil,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"610\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"690\">acumularam</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"690\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">baixa</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"850\">de</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"850\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"930\">70%</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"930\">SYM</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">ao</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1090\">ano.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1090\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1170\">Assim</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1170\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">já</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">ADV</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1330\">devolveram</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1330\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1410\">todos,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1410\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"247.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">eita.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-0\" stroke-width=\"2px\" d=\"M70,202.0 C70,162.0 110.0,162.0 110.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,204.0 L62,192.0 78,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-1\" stroke-width=\"2px\" d=\"M150,202.0 C150,2.0 690.0,2.0 690.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M150,204.0 L142,192.0 158,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-2\" stroke-width=\"2px\" d=\"M230,202.0 C230,162.0 270.0,162.0 270.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M230,204.0 L222,192.0 238,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-3\" stroke-width=\"2px\" d=\"M150,202.0 C150,122.0 275.0,122.0 275.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M275.0,204.0 L283.0,192.0 267.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-4\" stroke-width=\"2px\" d=\"M310,202.0 C310,162.0 350.0,162.0 350.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">appos</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M350.0,204.0 L358.0,192.0 342.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-5\" stroke-width=\"2px\" d=\"M310,202.0 C310,122.0 435.0,122.0 435.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">flat:name</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M435.0,204.0 L443.0,192.0 427.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-6\" stroke-width=\"2px\" d=\"M310,202.0 C310,82.0 520.0,82.0 520.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M520.0,204.0 L528.0,192.0 512.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-7\" stroke-width=\"2px\" d=\"M310,202.0 C310,42.0 605.0,42.0 605.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M605.0,204.0 L613.0,192.0 597.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-8\" stroke-width=\"2px\" d=\"M710,202.0 C710,162.0 750.0,162.0 750.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M750.0,204.0 L758.0,192.0 742.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-9\" stroke-width=\"2px\" d=\"M870,202.0 C870,162.0 910.0,162.0 910.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M870,204.0 L862,192.0 878,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-10\" stroke-width=\"2px\" d=\"M790,202.0 C790,122.0 915.0,122.0 915.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M915.0,204.0 L923.0,192.0 907.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-11\" stroke-width=\"2px\" d=\"M1030,202.0 C1030,162.0 1070.0,162.0 1070.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1030,204.0 L1022,192.0 1038,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-12\" stroke-width=\"2px\" d=\"M790,202.0 C790,82.0 1080.0,82.0 1080.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1080.0,204.0 L1088.0,192.0 1072.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-13\" stroke-width=\"2px\" d=\"M1190,202.0 C1190,122.0 1315.0,122.0 1315.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1190,204.0 L1182,192.0 1198,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-14\" stroke-width=\"2px\" d=\"M1270,202.0 C1270,162.0 1310.0,162.0 1310.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1270,204.0 L1262,192.0 1278,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-15\" stroke-width=\"2px\" d=\"M1350,202.0 C1350,162.0 1390.0,162.0 1390.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1390.0,204.0 L1398.0,192.0 1382.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-340775e57957462ea1bf746618bb93d0-0-16\" stroke-width=\"2px\" d=\"M1350,202.0 C1350,122.0 1475.0,122.0 1475.0,202.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-340775e57957462ea1bf746618bb93d0-0-16\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1475.0,204.0 L1483.0,192.0 1467.0,192.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adicionar e remover pipelines"
      ],
      "metadata": {
        "id": "iaF3eNKSsq9f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Pipeline Normal:\", nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vFCilfp9oU5-",
        "outputId": "493f96d4-8ab6-4dc7-fdcf-28a0927b6004"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline Normal: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.remove_pipe(\"ner\")\n",
        "print(\"Pipeline Atualizada:\", nlp.pipe_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "oI-jJwbEoifm",
        "outputId": "5526a919-e33b-4c9a-98e3-b382f53eef2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E001] No component 'ner' found in pipeline. Available names: []",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-e46cfaf7fd57>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline Atualizada:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36mremove_pipe\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \"\"\"\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m         \u001b[0mremoved\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_components\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;31m# We're only removing the component itself from the metas/configs here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: [E001] No component 'ner' found in pipeline. Available names: []"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.add_pipe(\"ner\", after=\"morphologizer\")\n",
        "print(\"Pipeline Atualizada:\", nlp.pipe_names)"
      ],
      "metadata": {
        "id": "G4xOnBEfpzlH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304
        },
        "outputId": "87da72ed-3d44-477f-ab9c-d6ef1b8754d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "[E001] No component 'morphologizer' found in pipeline. Available names: []",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-974f54436c84>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"morphologizer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pipeline Atualizada:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipe_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36madd_pipe\u001b[0;34m(self, factory_name, name, before, after, first, last, source, config, raw_config, validate)\u001b[0m\n\u001b[1;32m    826\u001b[0m                 \u001b[0mvalidate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             )\n\u001b[0;32m--> 828\u001b[0;31m         \u001b[0mpipe_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_pipe_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbefore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pipe_meta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_factory_meta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfactory_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_components\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe_component\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m_get_pipe_index\u001b[0;34m(self, before, after, first, last)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mafter\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    868\u001b[0m                     \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mafter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomponent_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m                 )\n",
            "\u001b[0;31mValueError\u001b[0m: [E001] No component 'morphologizer' found in pipeline. Available names: []"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLTK"
      ],
      "metadata": {
        "id": "0H6GmgQauN3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import RSLPStemmer, WordNetLemmatizer, SnowballStemmer, LancasterStemmer, PorterStemmer\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import pos_tag, pos_tag_sents\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIpH545GuPgy",
        "outputId": "a4954b7d-88d2-4d65-a937-3ebb82d9eafb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]   Package tagsets is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texto = \"Nós somos feitos de poeira de estrelas. Nós somos uma maneira de o cosmos se autoconhecer. A imaginação nos leva a mundos que nunca sequer existiram. Mas sem ela não vamos a lugar algum. \"\n",
        "print(texto)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvrNwDqrvI8A",
        "outputId": "3d40594e-5a15-4174-8090-4789ff2eba6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nós somos feitos de poeira de estrelas. Nós somos uma maneira de o cosmos se autoconhecer. A imaginação nos leva a mundos que nunca sequer existiram. Mas sem ela não vamos a lugar algum. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"maxent_ne_chunker_tab\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASymcQGkwVmB",
        "outputId": "65edc07d-7c65-4048-d0bd-3b75913c9fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentencas = sent_tokenize(texto, language=\"portuguese\")\n",
        "print(type(sentencas))\n",
        "print(sentencas)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lhxqENTUwyme",
        "outputId": "14391165-3069-44fc-f406-8b5cc7b41e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "['Nós somos feitos de poeira de estrelas.', 'Nós somos uma maneira de o cosmos se autoconhecer.', 'A imaginação nos leva a mundos que nunca sequer existiram.', 'Mas sem ela não vamos a lugar algum.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(sentencas))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RuLeqg5-SeXx",
        "outputId": "b2e99adf-7e8c-4761-aa22-b3113fdb0def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = word_tokenize(texto, language=\"portuguese\")\n",
        "print(tokens)\n",
        "print(len(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3aTZxEwSiOB",
        "outputId": "bb554c08-23d9-4274-d943-5cd60976ad19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nós', 'somos', 'feitos', 'de', 'poeira', 'de', 'estrelas', '.', 'Nós', 'somos', 'uma', 'maneira', 'de', 'o', 'cosmos', 'se', 'autoconhecer', '.', 'A', 'imaginação', 'nos', 'leva', 'a', 'mundos', 'que', 'nunca', 'sequer', 'existiram', '.', 'Mas', 'sem', 'ela', 'não', 'vamos', 'a', 'lugar', 'algum', '.']\n",
            "38\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stops = stopwords.words(\"portuguese\")\n",
        "print(len(stops))\n",
        "print(stops)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6538YhEDTuyp",
        "outputId": "6696d36b-0f77-4e04-e27a-136c9d6f9a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "207\n",
            "['a', 'à', 'ao', 'aos', 'aquela', 'aquelas', 'aquele', 'aqueles', 'aquilo', 'as', 'às', 'até', 'com', 'como', 'da', 'das', 'de', 'dela', 'delas', 'dele', 'deles', 'depois', 'do', 'dos', 'e', 'é', 'ela', 'elas', 'ele', 'eles', 'em', 'entre', 'era', 'eram', 'éramos', 'essa', 'essas', 'esse', 'esses', 'esta', 'está', 'estamos', 'estão', 'estar', 'estas', 'estava', 'estavam', 'estávamos', 'este', 'esteja', 'estejam', 'estejamos', 'estes', 'esteve', 'estive', 'estivemos', 'estiver', 'estivera', 'estiveram', 'estivéramos', 'estiverem', 'estivermos', 'estivesse', 'estivessem', 'estivéssemos', 'estou', 'eu', 'foi', 'fomos', 'for', 'fora', 'foram', 'fôramos', 'forem', 'formos', 'fosse', 'fossem', 'fôssemos', 'fui', 'há', 'haja', 'hajam', 'hajamos', 'hão', 'havemos', 'haver', 'hei', 'houve', 'houvemos', 'houver', 'houvera', 'houverá', 'houveram', 'houvéramos', 'houverão', 'houverei', 'houverem', 'houveremos', 'houveria', 'houveriam', 'houveríamos', 'houvermos', 'houvesse', 'houvessem', 'houvéssemos', 'isso', 'isto', 'já', 'lhe', 'lhes', 'mais', 'mas', 'me', 'mesmo', 'meu', 'meus', 'minha', 'minhas', 'muito', 'na', 'não', 'nas', 'nem', 'no', 'nos', 'nós', 'nossa', 'nossas', 'nosso', 'nossos', 'num', 'numa', 'o', 'os', 'ou', 'para', 'pela', 'pelas', 'pelo', 'pelos', 'por', 'qual', 'quando', 'que', 'quem', 'são', 'se', 'seja', 'sejam', 'sejamos', 'sem', 'ser', 'será', 'serão', 'serei', 'seremos', 'seria', 'seriam', 'seríamos', 'seu', 'seus', 'só', 'somos', 'sou', 'sua', 'suas', 'também', 'te', 'tem', 'tém', 'temos', 'tenha', 'tenham', 'tenhamos', 'tenho', 'terá', 'terão', 'terei', 'teremos', 'teria', 'teriam', 'teríamos', 'teu', 'teus', 'teve', 'tinha', 'tinham', 'tínhamos', 'tive', 'tivemos', 'tiver', 'tivera', 'tiveram', 'tivéramos', 'tiverem', 'tivermos', 'tivesse', 'tivessem', 'tivéssemos', 'tu', 'tua', 'tuas', 'um', 'uma', 'você', 'vocês', 'vos']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "palavras_sem_stopwords = [palavra for palavra in tokens if not palavra in stops]\n",
        "print(len(palavras_sem_stopwords))\n",
        "print(texto)\n",
        "print(palavras_sem_stopwords)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7K58vE4VUl-g",
        "outputId": "bca08f12-cb06-47ab-b555-5b1c2db14efd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23\n",
            "Nós somos feitos de poeira de estrelas. Nós somos uma maneira de o cosmos se autoconhecer. A imaginação nos leva a mundos que nunca sequer existiram. Mas sem ela não vamos a lugar algum. \n",
            "['Nós', 'feitos', 'poeira', 'estrelas', '.', 'Nós', 'maneira', 'cosmos', 'autoconhecer', '.', 'A', 'imaginação', 'leva', 'mundos', 'nunca', 'sequer', 'existiram', '.', 'Mas', 'vamos', 'lugar', 'algum', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RX-_a0nBVyUQ",
        "outputId": "da15c573-3bad-48ac-a969-b1532daf8aa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "palavras_sem_pontuacao = [palavra for palavra in palavras_sem_stopwords if not palavra in string.punctuation]\n",
        "print(texto)\n",
        "print(len(palavras_sem_pontuacao))\n",
        "print(palavras_sem_pontuacao)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M82LGCpzXI26",
        "outputId": "1c4cc00b-6608-4d1d-e39f-0712271b1266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nós somos feitos de poeira de estrelas. Nós somos uma maneira de o cosmos se autoconhecer. A imaginação nos leva a mundos que nunca sequer existiram. Mas sem ela não vamos a lugar algum. \n",
            "19\n",
            "['Nós', 'feitos', 'poeira', 'estrelas', 'Nós', 'maneira', 'cosmos', 'autoconhecer', 'A', 'imaginação', 'leva', 'mundos', 'nunca', 'sequer', 'existiram', 'Mas', 'vamos', 'lugar', 'algum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Frequência dos Tokens"
      ],
      "metadata": {
        "id": "EM7shvPKqN2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "frequencia = nltk.FreqDist(palavras_sem_pontuacao)\n",
        "frequencia"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CR2pjCZHXf1l",
        "outputId": "ee5c86dd-6023-463a-f29e-4eb10d074214"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FreqDist({'Nós': 2, 'feitos': 1, 'poeira': 1, 'estrelas': 1, 'maneira': 1, 'cosmos': 1, 'autoconhecer': 1, 'A': 1, 'imaginação': 1, 'leva': 1, ...})"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mais_comuns = frequencia.most_common(5)\n",
        "mais_comuns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VN0VUMULrHiQ",
        "outputId": "96ab7511-a761-4001-d28c-f96a3871d3ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Nós', 2), ('feitos', 1), ('poeira', 1), ('estrelas', 1), ('maneira', 1)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "stem1 = [stemmer.stem(word) for word in palavras_sem_pontuacao]\n",
        "print(palavras_sem_pontuacao)\n",
        "print(stem1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIV5rgG2rpax",
        "outputId": "273477b0-ea6d-47fc-d176-a03ad0f624a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nós', 'feitos', 'poeira', 'estrelas', 'Nós', 'maneira', 'cosmos', 'autoconhecer', 'A', 'imaginação', 'leva', 'mundos', 'nunca', 'sequer', 'existiram', 'Mas', 'vamos', 'lugar', 'algum']\n",
            "['nó', 'feito', 'poeira', 'estrela', 'nó', 'maneira', 'cosmo', 'autoconhec', 'a', 'imaginação', 'leva', 'mundo', 'nunca', 'sequer', 'existiram', 'ma', 'vamo', 'lugar', 'algum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer2 = SnowballStemmer(\"portuguese\")\n",
        "stem2 = [stemmer2.stem(word) for word in palavras_sem_pontuacao]\n",
        "print(palavras_sem_pontuacao)\n",
        "print(stem2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpqTVIn0tE0e",
        "outputId": "495aa7f9-f2da-4a4e-f4ed-82ddee2a8ffa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nós', 'feitos', 'poeira', 'estrelas', 'Nós', 'maneira', 'cosmos', 'autoconhecer', 'A', 'imaginação', 'leva', 'mundos', 'nunca', 'sequer', 'existiram', 'Mas', 'vamos', 'lugar', 'algum']\n",
            "['nós', 'feit', 'poeir', 'estrel', 'nós', 'maneir', 'cosm', 'autoconhec', 'a', 'imagin', 'lev', 'mund', 'nunc', 'sequ', 'exist', 'mas', 'vam', 'lug', 'algum']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer3 = LancasterStemmer()\n",
        "stem3 = [stemmer3.stem(word) for word in palavras_sem_pontuacao]\n",
        "print(palavras_sem_pontuacao)\n",
        "print(stem3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sbNWOGwnt2kI",
        "outputId": "425417be-6d6d-46d7-c3af-0f9080b85eec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nós', 'feitos', 'poeira', 'estrelas', 'Nós', 'maneira', 'cosmos', 'autoconhecer', 'A', 'imaginação', 'leva', 'mundos', 'nunca', 'sequer', 'existiram', 'Mas', 'vamos', 'lugar', 'algum']\n",
            "['nós', 'feito', 'poeir', 'estrela', 'nós', 'maneir', 'cosmo', 'autoconhec', 'a', 'imaginação', 'lev', 'mundo', 'nunc', 'sequ', 'existiram', 'mas', 'vamo', 'lug', 'alg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilize caso tenha alguma dúvida sobre marcação"
      ],
      "metadata": {
        "id": "y-bLxced73BP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "nltk.help.upenn_tagset()"
      ],
      "metadata": {
        "id": "heYXgaoQ7SGk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "outputId": "7571eabb-14e0-44a5-c19d-25053c141aa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mtagsets_json\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets_json')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mhelp/tagsets_json/PY3_json/upenn_tagset.json\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-76-6efdd56c3214>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhelp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupenn_tagset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/help.py\u001b[0m in \u001b[0;36mupenn_tagset\u001b[0;34m(tagpattern)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mupenn_tagset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_format_tagset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"upenn_tagset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/help.py\u001b[0m in \u001b[0;36m_format_tagset\u001b[0;34m(tagset, tagpattern)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_format_tagset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagpattern\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Load tagset from json file.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mtag_json_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"help/tagsets_json/PY3_json/{tagset}.json\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_json_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mtagdict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mtagsets_json\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('tagsets_json')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mhelp/tagsets_json/PY3_json/upenn_tagset.json\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Somnte suporte inglês ou Russo\n",
        "pos = nltk.pos_tag(palavras_sem_pontuacao, lang=\"eng\")\n",
        "print(pos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 686
        },
        "id": "eJ47GalO9tpr",
        "outputId": "f1c9aa65-ad96-4a55-8d36-fda50a2cfc5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-8733bbcc077a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Somnte suporte inglês ou Russo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpalavras_sem_pontuacao\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tagdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Marcação de cada Token"
      ],
      "metadata": {
        "id": "KLBp2PXuBeY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token2 = sent_tokenize(texto)\n",
        "ntokens = []\n",
        "for tokensentenca in token2:\n",
        "  ntokens.append(word_tokenize(tokensentenca))\n",
        "print(ntokens)\n",
        "\n",
        "possentenca = pos_tag_sents(ntokens)\n",
        "print(possentenca)"
      ],
      "metadata": {
        "id": "2JyGksgj_U7w",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 741
        },
        "outputId": "39f7bde0-62af-41bd-eec4-6a0c1f85c39b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['Nós', 'somos', 'feitos', 'de', 'poeira', 'de', 'estrelas', '.'], ['Nós', 'somos', 'uma', 'maneira', 'de', 'o', 'cosmos', 'se', 'autoconhecer', '.'], ['A', 'imaginação', 'nos', 'leva', 'a', 'mundos', 'que', 'nunca', 'sequer', 'existiram', '.'], ['Mas', 'sem', 'ela', 'não', 'vamos', 'a', 'lugar', 'algum', '.']]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-71-c89c51d1c988>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mpossentenca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag_sents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mntokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossentenca\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag_sents\u001b[0;34m(sentences, tagset, lang)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \"\"\"\n\u001b[0;32m--> 186\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    187\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tagdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Criação de lemas"
      ],
      "metadata": {
        "id": "XVEU4_0SBlHt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "reultado = [lemmatizer.lemmatize(palavra) for palavra in palavras_sem_stopwords]\n",
        "print(palavras_sem_pontuacao)\n",
        "print(reultado)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRtdVmKRBkHv",
        "outputId": "ccdab9bd-52b2-488a-c6a3-2b9729f110bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Nós', 'feitos', 'poeira', 'estrelas', 'Nós', 'maneira', 'cosmos', 'autoconhecer', 'A', 'imaginação', 'leva', 'mundos', 'nunca', 'sequer', 'existiram', 'Mas', 'vamos', 'lugar', 'algum']\n",
            "['Nós', 'feitos', 'poeira', 'estrelas', '.', 'Nós', 'maneira', 'cosmos', 'autoconhecer', '.', 'A', 'imaginação', 'lev', 'mundos', 'nunca', 'sequer', 'existiram', '.', 'Mas', 'vamos', 'lugar', 'algum', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entidades nomeadas"
      ],
      "metadata": {
        "id": "ilwNals9Cd0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texto_en = \"Barack Obama foi um presidente dos EUA\"\n",
        "token3 = word_tokenize(texto_en)\n",
        "tags = pos_tag(token3)\n",
        "en = nltk.ne_chunk(tags)\n",
        "print(en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 721
        },
        "id": "65SkCInrCg7A",
        "outputId": "463e877a-4c2d-4b1c-cae8-71dc4e5af95c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-77-47b905a480ce>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtexto_en\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Barack Obama foi um presidente dos EUA\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtoken3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexto_en\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0men\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mne_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36mpos_tag\u001b[0;34m(tokens, tagset, lang)\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_tagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_pos_tag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/__init__.py\u001b[0m in \u001b[0;36m_get_tagger\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mtagger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPerceptronTagger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, load, lang)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_conf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_tagdict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/tag/perceptron.py\u001b[0m in \u001b[0;36mload_from_json\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_from_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"eng\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0;31m# Automatically find path to the tagger if location is not specified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"taggers/averaged_perceptron_tagger_{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mTAGGER_JSONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"weights\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GwvtlumXp6Nf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}